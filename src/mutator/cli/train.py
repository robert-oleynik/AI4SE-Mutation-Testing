import pathlib

import click


@click.command(help="Train a LLM model based on the data generated by `collect`")
@click.option(
    "-o",
    "--out-dir",
    default=pathlib.Path("out", "model"),
    type=pathlib.Path,
    show_default=True,
    help="Path to store trained model at",
)
@click.option(
    "-m",
    "--model-id",
    default="google/codegemma-1.1-2b",
    show_default=True,
    help="LLM model to train on",
)
@click.option(
    "--device",
    default="cuda:0",
    show_default=True,
    help="GPU device to run LLM model on",
)
@click.option(
    "-d",
    "--dataset",
    default=pathlib.Path("out", "dataset", "data"),
    type=pathlib.Path,
    show_default=True,
    help="Dir to read datasets from",
)
@click.option("-g", "--generator", help="Generator format used for the dataset")
@click.option(
    "-l",
    "--learning-rate",
    default=3e-4,
    type=float,
    show_default=True,
    help="Learning rate used for LoRa training",
)
@click.option(
    "-n",
    "--num-epochs",
    default=2,
    type=int,
    show_default=True,
    help="Number of epochs to used for LoRa training",
)
@click.option(
    "-b",
    "--batch-size",
    default=4,
    type=int,
    show_default=True,
    help="Number of batches for LoRa training",
)
@click.option(
    "-t",
    "--test-size",
    default=0.1,
    type=float,
    show_default=True,
    help="Proportion of dataset used for testing/evaluation",
)
@click.option(
    "-s",
    "--seed",
    default=42,
    type=int,
    show_default=True,
    help="Seed used for randomness during training",
)
@click.option("--block-size", default=128, type=int, show_default=True, help="TODO")
@click.option("--alpha", default=16, type=int, show_default=True, help="TODO")
@click.option("--dropout", default=0.05, type=float, show_default=True, help="TODO")
@click.option("--r", default=8, type=int, show_default=True, help="TODO")
@click.option(
    "-w",
    "--warmup",
    default=0,
    type=int,
    show_default=True,
    help="Number of warmup steps to perform",
)
def train(
    out_dir,
    model_id,
    device,
    dataset,
    generator,
    learning_rate,
    num_epochs,
    batch_size,  # TODO
    test_size,
    seed,
    block_size,  # TODO
    alpha,
    dropout,
    r,
    warmup,  # TODO
):
    import datasets

    data = datasets.load_from_disk(dataset_path=dataset.absolute().__str__())
    data = datasets.Dataset.from_dict({"prompt": data["prompt"]})

    import mutator.ai.llm
    from mutator.ai.llm import LLM

    mutator.ai.llm = LLM(device, model_id)

    # From provided JuypterLab
    def _tokenize(row):
        tokenizer = mutator.ai.llm.tokenizer
        source = row["prompt"]

        input_ids = tokenizer.encode(source) + [tokenizer.eos_token_id]
        labels = input_ids.copy()
        return {"input_ids": input_ids, "labels": labels}

    def _block(data):
        tokenizer = mutator.ai.llm.tokenizer
        concatenated = sum(data["input_ids"], [])
        length = len(concatenated)

        truncated_length = (length // block_size) * block_size
        blocked_ids = [
            concatenated[i : i + block_size]
            for i in range(0, truncated_length, block_size)
        ]

        pad_length = block_size - (length % block_size)
        if pad_length != block_size:
            blocked_ids += [
                concatenated[truncated_length:] + [tokenizer.eos_token_id] * pad_length
            ]
        assert len(blocked_ids) > 0
        return {"input_ids": blocked_ids, "labels": blocked_ids.copy()}

    tokenized_data = data.map(_tokenize, remove_columns=["prompt"])

    split_dataset = tokenized_data.train_test_split(
        test_size=0.1, shuffle=True, seed=42
    )
    test_data = split_dataset["test"]
    train_data = split_dataset["train"]

    # test_data_blocks = test_data.map(_block, batched=True)
    # train_data_blocks = train_data.map(_block, batched=True)

    # Modified From: https://huggingface.co/docs/transformers/training
    import evaluate
    import numpy
    from peft import LoraConfig
    from transformers import Trainer, TrainingArguments

    model = mutator.ai.llm.model
    peft_config = LoraConfig(
        lora_alpha=alpha,
        lora_dropout=dropout,
        r=r,
        bias="none",
        task_type="CASUAL_LM",
    )
    model.add_adapter(peft_config)

    metric = evaluate.load("accuracy")

    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        predictions = numpy.argmax(logits, axis=-1)
        return metric.compute(predictions=predictions, references=labels)

    args = TrainingArguments(
        output_dir=out_dir / "train",
        eval_strategy="epoch",
        learning_rate=learning_rate,
        seed=seed,
    )

    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=train_data,
        eval_dataset=test_data,
        compute_metrics=compute_metrics,
    )
    trainer.train()

    model.save_pretrained(out_dir)
